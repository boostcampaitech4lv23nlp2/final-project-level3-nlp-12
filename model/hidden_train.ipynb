{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "import collections\n",
    "import torch, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 불러오기\n",
    "whisper_model = whisper.load_model('large')  # model_name : large(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "dataset_1 = load_dataset(\"anton-l/superb_demo\", \"er\", split=\"session1\")       # session1~5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = []\n",
    "# for data in dataset_1 :\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()    \n",
    "#     print(data['file'])\n",
    "#     audio = whisper.load_audio(data['file'])\n",
    "#     edited_data = whisper.pad_or_trim(audio)\n",
    "#     mel = whisper.log_mel_spectrogram(edited_data).to(whisper_model.device)\n",
    "#     # dim 맞추기\n",
    "#     mel = mel[None, :, :]       # torch.Size([1, 80, 3000])\n",
    "#     print(mel)\n",
    "#     output = whisper_model.encoder(mel)\n",
    "    \n",
    "#     test.append((output,data['label']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder output에\n",
    "# 평탄화(FC layer) -> 활성화함수 활성화 -> 분류기(softmax) 함수로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lv3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-01 05:55:29.982039: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-01 05:55:30.947563: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-01 05:55:30.947657: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-01 05:55:30.947665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "import collections\n",
    "import torch, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_model = whisper.load_model('large')  # model_name : large(default)\n",
    "encoder = w_model.encoder.to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioEncoder(\n",
       "  (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "  (blocks): ModuleList(\n",
       "    (0): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (12): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (13): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (14): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (15): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (16): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (17): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (18): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (19): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (20): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (21): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (22): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (23): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (24): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (25): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (26): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (27): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (28): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (29): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (30): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (31): ResidualAttentionBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "      )\n",
       "      (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm\n",
    "# 각 인스턴스에 나온 feature을 모든 channel에 걸쳐 한번에 normalize하는 것\n",
    "# CV에서는 Batch Norm, NLP에서는 LayerNorm을 표준으로 사용함\n",
    "# >> NLP의 경우, 입력 길이의 가변성, 미니 배치 분포의 심한 변동성, RNN모델에 BN 개념의 모호성 때문에 LN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier  = nn.Sequential(\n",
    "    nn.Linear(1280, 512),   # FC 1\n",
    "    nn.GELU(approximate='none'),\n",
    "    nn.Linear(512, 7)        # FC 2\n",
    ")\n",
    "\n",
    "model = nn.Sequential(collections.OrderedDict([\n",
    "    ('encoder', encoder),\n",
    "    ('classifier', my_classifier)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = whisper.load_audio(dataset_1['file'][0])\n",
    "edited_data = whisper.pad_or_trim(audio)\n",
    "mel = whisper.log_mel_spectrogram(edited_data).to(whisper_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = whisper_model.encoder(mel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for data in dataset_1 :\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()    \n",
    "    print(data['file'])\n",
    "    audio = whisper.load_audio(data['file'])\n",
    "    edited_data = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(edited_data).to(whisper_model.device)\n",
    "    # dim 맞추기\n",
    "    mel = mel[None, :, :]       # torch.Size([1, 80, 3000])\n",
    "    print(mel)\n",
    "    output = whisper_model.encoder(mel)\n",
    "    \n",
    "    test.append((output,data['label']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "\n",
    "input_file = '/opt/ml/final/honeybee.wav'\n",
    "w_model = whisper.load_model('large')  # model_name : large(default)\n",
    "w_model = model.to(torch.device(\"cuda\"))\n",
    "result = model.transcribe(input_file,fp16=False,language='English')\n",
    "\n",
    "def split_wav(data, sample_rate, start, end):\n",
    "    start *= sample_rate\n",
    "    end *= sample_rate\n",
    "    return data[start:end]\n",
    "\n",
    "def split_run(input_file,time_list):\n",
    "    audio = whisper.load_audio(input_file)  # 원본 wav 파일\n",
    "    edited_audio = []\n",
    "    \n",
    "    for start,end,text in time_list :\n",
    "        # start초 end초 까지의 데이터 추출  : array([0., 0., 0., ..., 0., 0., 0.], dtype=float32) 형태\n",
    "        edited_audio.append(split_wav(audio, 16000, int(start), int(end)))\n",
    "    return edited_audio\n",
    "\n",
    "timeline = []\n",
    "for line in result[\"segments\"]:\n",
    "    timeline.append([line['start'], line['end'],line['text']])          # [ start,end,text ] :str\n",
    "\n",
    "audios = split_run(input_file, timeline)\n",
    "audios\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50d8937722835aad8e9e88a5d4c447cea65bda64e03a350a97e9a5eec42c80db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
