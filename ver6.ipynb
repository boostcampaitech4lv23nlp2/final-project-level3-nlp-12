{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lv3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "audio = whisper.load_audio(\"/opt/ml/final_test_0127/dataset/test03.mp3\")   # 1분 50초 동영상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('/opt/ml/final/pl/datamodule/iemocap_full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/whisper/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m audio_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m emo_selected[\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m----> 5\u001b[0m         audio \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39;49mload_audio(path)\n\u001b[1;32m      6\u001b[0m         audio \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39mpad_or_trim(audio)    \n\u001b[1;32m      7\u001b[0m         mel \u001b[39m=\u001b[39m whisper\u001b[39m.\u001b[39mlog_mel_spectrogram(audio)\n",
      "File \u001b[0;32m/opt/conda/envs/whisper/lib/python3.9/site-packages/whisper/audio.py:42\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mOpen an audio file and read as mono waveform, resampling as necessary\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mA NumPy array containing the audio waveform, in float32 dtype.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39m# This launches a subprocess to decode audio while down-mixing and resampling as necessary.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39m# Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     out, _ \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 42\u001b[0m         ffmpeg\u001b[39m.\u001b[39;49minput(file, threads\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     43\u001b[0m         \u001b[39m.\u001b[39;49moutput(\u001b[39m\"\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39ms16le\u001b[39;49m\u001b[39m\"\u001b[39;49m, acodec\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpcm_s16le\u001b[39;49m\u001b[39m\"\u001b[39;49m, ac\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, ar\u001b[39m=\u001b[39;49msr)\n\u001b[1;32m     44\u001b[0m         \u001b[39m.\u001b[39;49mrun(cmd\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mffmpeg\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m-nostdin\u001b[39;49m\u001b[39m\"\u001b[39;49m], capture_stdout\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, capture_stderr\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m ffmpeg\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to load audio: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mdecode()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/whisper/lib/python3.9/site-packages/ffmpeg/_run.py:313\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(stream_spec, cmd, capture_stdout, capture_stderr, input, quiet, overwrite_output)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m@output_operator\u001b[39m()\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\n\u001b[1;32m    291\u001b[0m     stream_spec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     overwrite_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    298\u001b[0m ):\n\u001b[1;32m    299\u001b[0m     \u001b[39m\"\"\"Invoke ffmpeg for the supplied node graph.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m    Returns: (out, err) tuple containing captured stdout and stderr data.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m     process \u001b[39m=\u001b[39m run_async(\n\u001b[1;32m    314\u001b[0m         stream_spec,\n\u001b[1;32m    315\u001b[0m         cmd,\n\u001b[1;32m    316\u001b[0m         pipe_stdin\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    317\u001b[0m         pipe_stdout\u001b[39m=\u001b[39;49mcapture_stdout,\n\u001b[1;32m    318\u001b[0m         pipe_stderr\u001b[39m=\u001b[39;49mcapture_stderr,\n\u001b[1;32m    319\u001b[0m         quiet\u001b[39m=\u001b[39;49mquiet,\n\u001b[1;32m    320\u001b[0m         overwrite_output\u001b[39m=\u001b[39;49moverwrite_output,\n\u001b[1;32m    321\u001b[0m     )\n\u001b[1;32m    322\u001b[0m     out, err \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mcommunicate(\u001b[39minput\u001b[39m)\n\u001b[1;32m    323\u001b[0m     retcode \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mpoll()\n",
      "File \u001b[0;32m/opt/conda/envs/whisper/lib/python3.9/site-packages/ffmpeg/_run.py:284\u001b[0m, in \u001b[0;36mrun_async\u001b[0;34m(stream_spec, cmd, pipe_stdin, pipe_stdout, pipe_stderr, quiet, overwrite_output)\u001b[0m\n\u001b[1;32m    282\u001b[0m stdout_stream \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPIPE \u001b[39mif\u001b[39;00m pipe_stdout \u001b[39mor\u001b[39;00m quiet \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    283\u001b[0m stderr_stream \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPIPE \u001b[39mif\u001b[39;00m pipe_stderr \u001b[39mor\u001b[39;00m quiet \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m subprocess\u001b[39m.\u001b[39;49mPopen(\n\u001b[1;32m    285\u001b[0m     args, stdin\u001b[39m=\u001b[39;49mstdin_stream, stdout\u001b[39m=\u001b[39;49mstdout_stream, stderr\u001b[39m=\u001b[39;49mstderr_stream\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/whisper/lib/python3.9/subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    948\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    949\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 951\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    952\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    953\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    954\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    955\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    956\u001b[0m                         errread, errwrite,\n\u001b[1;32m    957\u001b[0m                         restore_signals,\n\u001b[1;32m    958\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    959\u001b[0m                         start_new_session)\n\u001b[1;32m    960\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/opt/conda/envs/whisper/lib/python3.9/subprocess.py:1777\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1775\u001b[0m errpipe_data \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m()\n\u001b[1;32m   1776\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1777\u001b[0m     part \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mread(errpipe_read, \u001b[39m50000\u001b[39;49m)\n\u001b[1;32m   1778\u001b[0m     errpipe_data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m part\n\u001b[1;32m   1779\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m part \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(errpipe_data) \u001b[39m>\u001b[39m \u001b[39m50000\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emo = ['neu', 'hap', 'sad', 'ang', 'fea']\n",
    "emo_selected = dataset.loc[dataset.emotion.isin(emo)]\n",
    "audio_list = []\n",
    "for path in emo_selected['path']:\n",
    "        audio = whisper.load_audio(path)\n",
    "        audio = whisper.pad_or_trim(audio)    \n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        mel = mel[None, :, :]  \n",
    "        audio_list.append(mel.tolist())\n",
    "out_dataset = pd.DataFrame(\n",
    "        {\n",
    "            \"audio\" : audio_list,\n",
    "            \"label\": dataset[\"label\"],\n",
    "        }\n",
    "    )\n",
    "out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dataset['audio'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "import os\n",
    "for p in dataset['path']:\n",
    "    path = p.split('/')[-1]\n",
    "    path = path.split('.')[0]\n",
    "    tmp.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['path'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('/opt/ml/final/pl/datamodule/iemocap_full_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio   #(1779206,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2525184,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 0.07836914, 0.07250977,\n",
       "       0.03768921], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오디오 입력의 샘플링 속도를 모델에서 예상하느 샘플링 속도와 일치시키기\n",
    "# Whisper_feature_extractor는 2가지 작업을 수행해야함.\n",
    "# 1-1. 모든 샘플의 입력길이가 30초가 되도록 오디오 샘플 배치를 채우거나 자르기(padding)\n",
    "# 1-2. 1-1번에서 작업한 패딘된 오디오 array를 log-Mel spectrogram으로 변환\n",
    "\n",
    "# 1-1. 30초로 분활되어 패딩 및 자르기\n",
    "audio = whisper.pad_or_trim(audio)          \n",
    "audio       # audio.len : (480000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6634, -0.6634, -0.0352,  ...,  0.1270,  0.3081,  0.2873],\n",
       "        [-0.6634, -0.6634, -0.0857,  ...,  0.0288,  0.2825,  0.2265],\n",
       "        [-0.6634, -0.6634, -0.1781,  ...,  0.0614,  0.0889,  0.3709],\n",
       "        ...,\n",
       "        [-0.6634, -0.6546,  0.0634,  ...,  0.0476,  0.0112, -0.0835],\n",
       "        [-0.6634, -0.6634, -0.1304,  ..., -0.0162,  0.0729, -0.0101],\n",
       "        [-0.6634, -0.6634, -0.2362,  ..., -0.0752,  0.0377, -0.0628]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-2. log-Mel spectrogram으로 변환 후 모델로 전달\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "mel     # torch.Size([80, 3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1422,  0.7148, -0.4809,  ...,  0.2073,  1.0649, -0.1069],\n",
       "         [ 0.8354,  0.6639, -0.4328,  ..., -0.0761,  1.1189, -0.3803],\n",
       "         [ 0.2717,  0.6384, -0.0670,  ..., -0.5671, -0.1905, -1.0390],\n",
       "         ...,\n",
       "         [ 0.5807, -0.5485, -2.2702,  ..., -1.6035, -0.6112, -0.4250],\n",
       "         [ 0.2030, -0.2901, -1.1699,  ..., -0.9606, -0.5970, -1.1560],\n",
       "         [-0.7661,  1.0489, -0.2621,  ..., -0.1690, -0.4695, -0.8974]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델의 encoder에 mel tensor 입력하기\n",
    "\n",
    "# dim 맞추기\n",
    "mel = mel[None, :, :]       # torch.Size([1, 80, 3000])\n",
    "encoder_output = model.embed_audio(mel)\n",
    "encoder_output              # torch.Size([1, 1500, 512])    (batch_size, 1500(number of spectrogram), hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d92da0a1b2a9422f3c47c40724438aa15fd669204e554153def5387194ea7e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
